{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9392722,"sourceType":"datasetVersion","datasetId":5700165}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision torchaudio soundfile","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:35:19.068358Z","iopub.execute_input":"2024-09-15T04:35:19.069084Z","iopub.status.idle":"2024-09-15T04:35:32.772768Z","shell.execute_reply.started":"2024-09-15T04:35:19.069046Z","shell.execute_reply":"2024-09-15T04:35:32.771780Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install git+https://github.com/openai/whisper.git","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:35:32.774654Z","iopub.execute_input":"2024-09-15T04:35:32.774987Z","iopub.status.idle":"2024-09-15T04:36:09.701856Z","shell.execute_reply.started":"2024-09-15T04:35:32.774946Z","shell.execute_reply":"2024-09-15T04:36:09.700745Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/whisper.git\n  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-a_db_jxg\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-a_db_jxg\n  Resolved https://github.com/openai/whisper.git to commit 279133e3107392276dc509148da1f41bfb532c7e\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20231117) (0.58.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20231117) (1.26.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20231117) (2.4.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20231117) (4.66.4)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from openai-whisper==20231117) (10.3.0)\nCollecting tiktoken (from openai-whisper==20231117)\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting triton>=2.0.0 (from openai-whisper==20231117)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton>=2.0.0->openai-whisper==20231117) (3.15.1)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->openai-whisper==20231117) (0.41.1)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper==20231117) (2024.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\nDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-whisper\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802823 sha256=bdf4de0fb976e9c0a0057495f6f798210eed6d0b13c5cd62be405ccf399320b9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xyn3ynao/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\nSuccessfully built openai-whisper\nInstalling collected packages: triton, tiktoken, openai-whisper\nSuccessfully installed openai-whisper-20231117 tiktoken-0.7.0 triton-3.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers datasets sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:09.703196Z","iopub.execute_input":"2024-09-15T04:36:09.703530Z","iopub.status.idle":"2024-09-15T04:36:22.638018Z","shell.execute_reply.started":"2024-09-15T04:36:09.703491Z","shell.execute_reply":"2024-09-15T04:36:22.636863Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:22.642212Z","iopub.execute_input":"2024-09-15T04:36:22.642548Z","iopub.status.idle":"2024-09-15T04:36:23.023778Z","shell.execute_reply.started":"2024-09-15T04:36:22.642508Z","shell.execute_reply":"2024-09-15T04:36:23.022991Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/financial-inclusion-speech-dataset/fisd-akuapim-twi-90p/data.csv', delimiter='\\t')","metadata":{"id":"8pV71le9L0hv","execution":{"iopub.status.busy":"2024-09-15T04:36:23.024891Z","iopub.execute_input":"2024-09-15T04:36:23.025295Z","iopub.status.idle":"2024-09-15T04:36:23.144704Z","shell.execute_reply.started":"2024-09-15T04:36:23.025262Z","shell.execute_reply":"2024-09-15T04:36:23.143910Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"7abZS9jML8lN","outputId":"07e920e1-93ac-4950-8058-5d33f456cbe8","execution":{"iopub.status.busy":"2024-09-15T04:36:23.145787Z","iopub.execute_input":"2024-09-15T04:36:23.146071Z","iopub.status.idle":"2024-09-15T04:36:23.165593Z","shell.execute_reply.started":"2024-09-15T04:36:23.146040Z","shell.execute_reply":"2024-09-15T04:36:23.164673Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                     Audio Filepath  \\\n0           0  lacuna-audios-train/akuapim-twi/audios/Akuapem...   \n1           1  lacuna-audios-train/akuapim-twi/audios/Akuapem...   \n2           2  lacuna-audios-train/akuapim-twi/audios/Akuapem...   \n3           3  lacuna-audios-train/akuapim-twi/audios/Akuapem...   \n4           4  lacuna-audios-train/akuapim-twi/audios/Akuapem...   \n\n                                 Transcription     Translation  \n0  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n1  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n2  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n3  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n4  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Audio Filepath</th>\n      <th>Transcription</th>\n      <th>Translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>lacuna-audios-train/akuapim-twi/audios/Akuapem...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>lacuna-audios-train/akuapim-twi/audios/Akuapem...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>lacuna-audios-train/akuapim-twi/audios/Akuapem...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>lacuna-audios-train/akuapim-twi/audios/Akuapem...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>lacuna-audios-train/akuapim-twi/audios/Akuapem...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Update the Audio Filepath column\ndata['Audio Filepath'] = '/kaggle/input/financial-inclusion-speech-dataset/fisd-akuapim-twi-90p/audios/' + data['Audio Filepath'].str.split('/').str[-1]","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:23.166776Z","iopub.execute_input":"2024-09-15T04:36:23.167059Z","iopub.status.idle":"2024-09-15T04:36:23.219495Z","shell.execute_reply.started":"2024-09-15T04:36:23.167005Z","shell.execute_reply":"2024-09-15T04:36:23.218695Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Drop the 'Unnamed: 0' column and the index column if it exists\ndata_cleaned = data.drop(columns=['Unnamed: 0'], errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:23.220627Z","iopub.execute_input":"2024-09-15T04:36:23.220954Z","iopub.status.idle":"2024-09-15T04:36:23.231533Z","shell.execute_reply.started":"2024-09-15T04:36:23.220908Z","shell.execute_reply":"2024-09-15T04:36:23.230630Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Optionally, reset the index if you want a clean index in the new DataFrame\ndata_cleaned.reset_index(drop=True, inplace=True)","metadata":{"id":"FOcertmgL8u1","execution":{"iopub.status.busy":"2024-09-15T04:36:23.232675Z","iopub.execute_input":"2024-09-15T04:36:23.233017Z","iopub.status.idle":"2024-09-15T04:36:23.240865Z","shell.execute_reply.started":"2024-09-15T04:36:23.232984Z","shell.execute_reply":"2024-09-15T04:36:23.240079Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data_cleaned.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"4Xi0njMCRDXF","outputId":"b1ce0375-86d3-47fd-8540-2e328b5625b6","execution":{"iopub.status.busy":"2024-09-15T04:36:23.244632Z","iopub.execute_input":"2024-09-15T04:36:23.244965Z","iopub.status.idle":"2024-09-15T04:36:23.257184Z","shell.execute_reply.started":"2024-09-15T04:36:23.244922Z","shell.execute_reply":"2024-09-15T04:36:23.256192Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                      Audio Filepath  \\\n0  /kaggle/input/financial-inclusion-speech-datas...   \n1  /kaggle/input/financial-inclusion-speech-datas...   \n2  /kaggle/input/financial-inclusion-speech-datas...   \n3  /kaggle/input/financial-inclusion-speech-datas...   \n4  /kaggle/input/financial-inclusion-speech-datas...   \n\n                                 Transcription     Translation  \n0  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n1  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n2  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n3  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  \n4  Nnipa yɛ bad (informal) Nnipa nnye (formal)  People are bad  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Audio Filepath</th>\n      <th>Transcription</th>\n      <th>Translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/financial-inclusion-speech-datas...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/financial-inclusion-speech-datas...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/financial-inclusion-speech-datas...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/financial-inclusion-speech-datas...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/financial-inclusion-speech-datas...</td>\n      <td>Nnipa yɛ bad (informal) Nnipa nnye (formal)</td>\n      <td>People are bad</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_cleaned.to_csv('cleaned_dataset.csv', index=False)","metadata":{"id":"YqwgQ6s2RHDS","execution":{"iopub.status.busy":"2024-09-15T04:36:23.258284Z","iopub.execute_input":"2024-09-15T04:36:23.258571Z","iopub.status.idle":"2024-09-15T04:36:23.452812Z","shell.execute_reply.started":"2024-09-15T04:36:23.258539Z","shell.execute_reply":"2024-09-15T04:36:23.451937Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# Step 1: Load the Dataset\ndataset = load_dataset('csv', data_files='cleaned_dataset.csv')","metadata":{"id":"WxEg2DWpRfhV","execution":{"iopub.status.busy":"2024-09-15T04:36:23.454272Z","iopub.execute_input":"2024-09-15T04:36:23.454593Z","iopub.status.idle":"2024-09-15T04:36:46.547738Z","shell.execute_reply.started":"2024-09-15T04:36:23.454559Z","shell.execute_reply":"2024-09-15T04:36:46.546990Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"202092c295164b5bb03388f3900bf98a"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:46.548771Z","iopub.execute_input":"2024-09-15T04:36:46.549322Z","iopub.status.idle":"2024-09-15T04:36:46.555406Z","shell.execute_reply.started":"2024-09-15T04:36:46.549288Z","shell.execute_reply":"2024-09-15T04:36:46.554398Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Audio Filepath', 'Transcription', 'Translation'],\n        num_rows: 25636\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess the Audio Files","metadata":{}},{"cell_type":"code","source":"def preprocess_audio(audio_filepath, target_sample_rate=16000, backend='soundfile'):\n    try:\n        audio, sample_rate = torchaudio.load(audio_filepath, backend=backend)\n        if sample_rate != target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n            audio = resampler(audio)\n        audio = audio.squeeze().numpy()\n        return audio\n    except FileNotFoundError:\n        print(f\"Error: File not found at {audio_filepath}\")\n        return None\n    except Exception as e:\n        print(f\"Error processing {audio_filepath}: {e}\")\n        return None\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:46.556604Z","iopub.execute_input":"2024-09-15T04:36:46.556932Z","iopub.status.idle":"2024-09-15T04:36:46.645680Z","shell.execute_reply.started":"2024-09-15T04:36:46.556890Z","shell.execute_reply":"2024-09-15T04:36:46.644744Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Preprocess Dataset","metadata":{}},{"cell_type":"code","source":"# Safe preprocessing function\ndef safe_preprocess(example):\n    # Check if critical fields are None, and skip processing if they are\n    if example['Audio Filepath'] is None or example['Transcription'] is None:\n        return None  # Skip this example by returning None\n    # Add your preprocessing logic here (e.g., tokenization, normalization)\n    # Example:\n    example['Transcription'] = example['Transcription'].lower()  # Lowercasing\n    return example  # Return processed example if valid\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:46.646913Z","iopub.execute_input":"2024-09-15T04:36:46.647521Z","iopub.status.idle":"2024-09-15T04:36:46.658136Z","shell.execute_reply.started":"2024-09-15T04:36:46.647485Z","shell.execute_reply":"2024-09-15T04:36:46.657341Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Map the safe_preprocess function over the dataset\nprocessed_dataset = dataset.map(safe_preprocess)\n\n# Filter out rows that returned None from safe_preprocess\nprocessed_dataset = processed_dataset.filter(lambda x: x is not None)\n\n# Now you can proceed with further processing","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:46.659119Z","iopub.execute_input":"2024-09-15T04:36:46.659403Z","iopub.status.idle":"2024-09-15T04:36:48.793814Z","shell.execute_reply.started":"2024-09-15T04:36:46.659371Z","shell.execute_reply":"2024-09-15T04:36:48.792962Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25636 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1aa213b0994643b67d508d628c4613"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/25636 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0e61d6a9874073942c7ad975f2ee3a"}},"metadata":{}}]},{"cell_type":"code","source":"processed_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:48.795081Z","iopub.execute_input":"2024-09-15T04:36:48.795372Z","iopub.status.idle":"2024-09-15T04:36:48.801198Z","shell.execute_reply.started":"2024-09-15T04:36:48.795339Z","shell.execute_reply":"2024-09-15T04:36:48.800337Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Audio Filepath', 'Transcription', 'Translation'],\n        num_rows: 25636\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Step 3: Split dataset into train and validation sets","metadata":{}},{"cell_type":"code","source":"# Assuming 'processed_dataset' is a DatasetDict, get the specific dataset (e.g., 'train')\ndataset = processed_dataset['train']  # or the relevant split\n\n# Perform train-test split on the individual dataset\nsplit_dataset = dataset.train_test_split(test_size=0.2)\n\n# If you need to update the DatasetDict with the split data\nprocessed_dataset['train'] = split_dataset['train']\nprocessed_dataset['test'] = split_dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:48.802682Z","iopub.execute_input":"2024-09-15T04:36:48.803063Z","iopub.status.idle":"2024-09-15T04:36:48.837459Z","shell.execute_reply.started":"2024-09-15T04:36:48.803002Z","shell.execute_reply":"2024-09-15T04:36:48.836766Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"processed_dataset['train']","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:48.838396Z","iopub.execute_input":"2024-09-15T04:36:48.838669Z","iopub.status.idle":"2024-09-15T04:36:48.844386Z","shell.execute_reply.started":"2024-09-15T04:36:48.838630Z","shell.execute_reply":"2024-09-15T04:36:48.843494Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Audio Filepath', 'Transcription', 'Translation'],\n    num_rows: 20508\n})"},"metadata":{}}]},{"cell_type":"code","source":"processed_dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:48.845712Z","iopub.execute_input":"2024-09-15T04:36:48.846068Z","iopub.status.idle":"2024-09-15T04:36:48.855614Z","shell.execute_reply.started":"2024-09-15T04:36:48.846011Z","shell.execute_reply":"2024-09-15T04:36:48.854780Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Audio Filepath', 'Transcription', 'Translation'],\n    num_rows: 5128\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Check the number of audio files and transcriptions\naudio_count = len(dataset['Audio Filepath'])\ntranscription_count = len(dataset['Transcription'])\n\n# Output the counts\nprint(f\"Number of audio files: {audio_count}\")\nprint(f\"Number of transcriptions: {transcription_count}\")\n\n# Check if the counts are equal\nare_lengths_equal = audio_count == transcription_count\nprint(f\"Are the lengths equal? {are_lengths_equal}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:48.856745Z","iopub.execute_input":"2024-09-15T04:36:48.857048Z","iopub.status.idle":"2024-09-15T04:36:49.226160Z","shell.execute_reply.started":"2024-09-15T04:36:48.856990Z","shell.execute_reply":"2024-09-15T04:36:49.225217Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Number of audio files: 25636\nNumber of transcriptions: 25636\nAre the lengths equal? True\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the number of audio files and transcriptions\naudio_count = len(processed_dataset['train']['Audio Filepath'])\ntranscription_count = len(processed_dataset['train']['Transcription'])\n\n# Output the counts\nprint(f\"Number of audio files: {audio_count}\")\nprint(f\"Number of transcriptions: {transcription_count}\")\n\n# Check if the counts are equal\nare_lengths_equal = audio_count == transcription_count\nprint(f\"Are the lengths equal? {are_lengths_equal}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:49.227354Z","iopub.execute_input":"2024-09-15T04:36:49.227665Z","iopub.status.idle":"2024-09-15T04:36:49.514135Z","shell.execute_reply.started":"2024-09-15T04:36:49.227631Z","shell.execute_reply":"2024-09-15T04:36:49.513194Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Number of audio files: 20508\nNumber of transcriptions: 20508\nAre the lengths equal? True\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the number of audio files and transcriptions\naudio_count = len(processed_dataset['test']['Audio Filepath'])\ntranscription_count = len(processed_dataset['test']['Transcription'])\n\n# Output the counts\nprint(f\"Number of audio files: {audio_count}\")\nprint(f\"Number of transcriptions: {transcription_count}\")\n\n# Check if the counts are equal\nare_lengths_equal = audio_count == transcription_count\nprint(f\"Are the lengths equal? {are_lengths_equal}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:49.515450Z","iopub.execute_input":"2024-09-15T04:36:49.515852Z","iopub.status.idle":"2024-09-15T04:36:49.592764Z","shell.execute_reply.started":"2024-09-15T04:36:49.515808Z","shell.execute_reply":"2024-09-15T04:36:49.591860Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Number of audio files: 5128\nNumber of transcriptions: 5128\nAre the lengths equal? True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 4: Initialize Whisper Processor and Model","metadata":{}},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:49.594112Z","iopub.execute_input":"2024-09-15T04:36:49.594502Z","iopub.status.idle":"2024-09-15T04:36:50.376181Z","shell.execute_reply.started":"2024-09-15T04:36:49.594459Z","shell.execute_reply":"2024-09-15T04:36:50.375419Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33294a36b1754e5db489576986e61733"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import WhisperTokenizer\n\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\",task=\"translate\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:50.377202Z","iopub.execute_input":"2024-09-15T04:36:50.377495Z","iopub.status.idle":"2024-09-15T04:36:54.876906Z","shell.execute_reply.started":"2024-09-15T04:36:50.377462Z","shell.execute_reply":"2024-09-15T04:36:54.876080Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ffaf42c6374a748934404533dc8d6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b26dcbe9b2946e5b35952abe8c87561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d35695c379084226854930e256963a24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a74356fab642459ca1f63335103d7690"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aea0c4a2785b4db0a7c082b7753d5aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693827b98b434544808ffc4c48c094a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc065fb7fa741259865cba66b10af36"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\",task=\"translate\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:54.878039Z","iopub.execute_input":"2024-09-15T04:36:54.878356Z","iopub.status.idle":"2024-09-15T04:36:56.367904Z","shell.execute_reply.started":"2024-09-15T04:36:54.878323Z","shell.execute_reply":"2024-09-15T04:36:56.366884Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(processed_dataset[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:56.369189Z","iopub.execute_input":"2024-09-15T04:36:56.369483Z","iopub.status.idle":"2024-09-15T04:36:56.374874Z","shell.execute_reply.started":"2024-09-15T04:36:56.369450Z","shell.execute_reply":"2024-09-15T04:36:56.373955Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"{'Audio Filepath': '/kaggle/input/financial-inclusion-speech-dataset/fisd-akuapim-twi-90p/audios/AkuapemTwiFm20-IMMODeKQ-Tmp010-xRUMe6.ogg', 'Transcription': 'apoobɔ nnye', 'Translation': 'Harassment/bullying is not good'}\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Audio\n\nprocessed_dataset = processed_dataset.cast_column(\"Audio Filepath\", Audio(sampling_rate=16000))","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:56.381075Z","iopub.execute_input":"2024-09-15T04:36:56.381606Z","iopub.status.idle":"2024-09-15T04:36:56.398686Z","shell.execute_reply.started":"2024-09-15T04:36:56.381570Z","shell.execute_reply":"2024-09-15T04:36:56.397984Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(processed_dataset[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:36:56.399563Z","iopub.execute_input":"2024-09-15T04:36:56.399820Z","iopub.status.idle":"2024-09-15T04:37:08.755278Z","shell.execute_reply.started":"2024-09-15T04:36:56.399791Z","shell.execute_reply":"2024-09-15T04:37:08.754236Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"{'Audio Filepath': {'path': '/kaggle/input/financial-inclusion-speech-dataset/fisd-akuapim-twi-90p/audios/AkuapemTwiFm20-IMMODeKQ-Tmp010-xRUMe6.ogg', 'array': array([-3.05175781e-05, -3.05175781e-05, -3.05175781e-05, ...,\n        3.49121094e-02,  3.27453613e-02,  4.23278809e-02]), 'sampling_rate': 16000}, 'Transcription': 'apoobɔ nnye', 'Translation': 'Harassment/bullying is not good'}\n","output_type":"stream"}]},{"cell_type":"code","source":"processed_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.756764Z","iopub.execute_input":"2024-09-15T04:37:08.757727Z","iopub.status.idle":"2024-09-15T04:37:08.765239Z","shell.execute_reply.started":"2024-09-15T04:37:08.757676Z","shell.execute_reply":"2024-09-15T04:37:08.764042Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Audio Filepath', 'Transcription', 'Translation'],\n        num_rows: 20508\n    })\n    test: Dataset({\n        features: ['Audio Filepath', 'Transcription', 'Translation'],\n        num_rows: 5128\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"processed_dataset.remove_columns(['Transcription'])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.766356Z","iopub.execute_input":"2024-09-15T04:37:08.766651Z","iopub.status.idle":"2024-09-15T04:37:08.813437Z","shell.execute_reply.started":"2024-09-15T04:37:08.766619Z","shell.execute_reply":"2024-09-15T04:37:08.812496Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Audio Filepath', 'Translation'],\n        num_rows: 20508\n    })\n    test: Dataset({\n        features: ['Audio Filepath', 'Translation'],\n        num_rows: 5128\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_dataset(batch):\n    # load and resample audio data from 48 to 16kHz\n    audio = batch[\"Audio Filepath\"]\n\n    # compute log-Mel input features from input audio array\n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n\n    # encode target text to label ids\n    batch[\"labels\"] = tokenizer(batch[\"Translation\"]).input_ids\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.814770Z","iopub.execute_input":"2024-09-15T04:37:08.815119Z","iopub.status.idle":"2024-09-15T04:37:08.821827Z","shell.execute_reply.started":"2024-09-15T04:37:08.815081Z","shell.execute_reply":"2024-09-15T04:37:08.821064Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"processed_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.822959Z","iopub.execute_input":"2024-09-15T04:37:08.823276Z","iopub.status.idle":"2024-09-15T04:37:08.835184Z","shell.execute_reply.started":"2024-09-15T04:37:08.823245Z","shell.execute_reply":"2024-09-15T04:37:08.834301Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Audio Filepath', 'Transcription', 'Translation'],\n        num_rows: 20508\n    })\n    test: Dataset({\n        features: ['Audio Filepath', 'Transcription', 'Translation'],\n        num_rows: 5128\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# processed_dataset =processed_dataset.remove_columns([\"Transcription\"])\n\n# print(processed_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.836325Z","iopub.execute_input":"2024-09-15T04:37:08.836664Z","iopub.status.idle":"2024-09-15T04:37:08.847898Z","shell.execute_reply.started":"2024-09-15T04:37:08.836625Z","shell.execute_reply":"2024-09-15T04:37:08.847075Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# from datasets import DatasetDict\n\n# start_index = 11300\n# end_index = 11420\n\n# # Function to filter out the specified indices\n# def filter_indices(dataset, start, end):\n#     # Create a list of indices to keep\n#     indices_to_keep = [i for i in range(len(dataset)) if i < start or i > end]\n#     return dataset.select(indices_to_keep)\n\n# # Remove the specified indices from the train dataset\n# processed_dataset['train'] = filter_indices(processed_dataset['train'], start_index, end_index)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.848912Z","iopub.execute_input":"2024-09-15T04:37:08.849222Z","iopub.status.idle":"2024-09-15T04:37:08.858473Z","shell.execute_reply.started":"2024-09-15T04:37:08.849190Z","shell.execute_reply":"2024-09-15T04:37:08.857471Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# # Optionally, print the new size of the train dataset\n# print(f\"New size of train dataset: {len(processed_dataset['train'])}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.859638Z","iopub.execute_input":"2024-09-15T04:37:08.859966Z","iopub.status.idle":"2024-09-15T04:37:08.868344Z","shell.execute_reply.started":"2024-09-15T04:37:08.859927Z","shell.execute_reply":"2024-09-15T04:37:08.867462Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Assuming `processed_dataset` is your DatasetDict\ntotal_examples = 10000\n\ntraining_examples = total_examples // 2  # 10254\n\nstart_index = 0\nend_index = start_index + training_examples - 1  # 30538\n\n# Select the training dataset\nprocessed_dataset['train'] = processed_dataset['train'].select(range(start_index, end_index + 1))\n\n# Print the size of the training dataset\nprint(f\"Size of the training dataset: {len(processed_dataset['train'])}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.869423Z","iopub.execute_input":"2024-09-15T04:37:08.869730Z","iopub.status.idle":"2024-09-15T04:37:08.883096Z","shell.execute_reply.started":"2024-09-15T04:37:08.869683Z","shell.execute_reply":"2024-09-15T04:37:08.882262Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Size of the training dataset: 5000\n","output_type":"stream"}]},{"cell_type":"code","source":"processed_dataset = processed_dataset.map(prepare_dataset, remove_columns=processed_dataset.column_names[\"train\"], num_proc=2)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:37:08.884084Z","iopub.execute_input":"2024-09-15T04:37:08.884345Z","iopub.status.idle":"2024-09-15T04:40:43.529089Z","shell.execute_reply.started":"2024-09-15T04:37:08.884316Z","shell.execute_reply":"2024-09-15T04:40:43.527983Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7b04751aac44d0a629b89de3cf9b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/5128 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"386982b31a7e4d348e92d436e090a089"}},"metadata":{}}]},{"cell_type":"code","source":"processed_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:40:43.530625Z","iopub.execute_input":"2024-09-15T04:40:43.530961Z","iopub.status.idle":"2024-09-15T04:40:43.537701Z","shell.execute_reply.started":"2024-09-15T04:40:43.530923Z","shell.execute_reply":"2024-09-15T04:40:43.536695Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_features', 'labels'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['input_features', 'labels'],\n        num_rows: 5128\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Filter out None entries\n# processed_dataset = processed_dataset.filter(lambda x: x is not None)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:40:43.538789Z","iopub.execute_input":"2024-09-15T04:40:43.539069Z","iopub.status.idle":"2024-09-15T04:40:47.778195Z","shell.execute_reply.started":"2024-09-15T04:40:43.539038Z","shell.execute_reply":"2024-09-15T04:40:47.777055Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:40:47.779580Z","iopub.execute_input":"2024-09-15T04:40:47.779899Z","iopub.status.idle":"2024-09-15T04:41:05.125769Z","shell.execute_reply.started":"2024-09-15T04:40:47.779866Z","shell.execute_reply":"2024-09-15T04:41:05.124889Z"},"trusted":true},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbfe6745f5f347aea5b5a2583bffb645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f83f1fef0b4c1ab0da3ceab6888b9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a1f1b51c7a14f658e2d9327bebff7e1"}},"metadata":{}}]},{"cell_type":"code","source":"# Configure the model's generation settings\nmodel.generation_config.task = \"translate\" \nmodel.generation_config.language = None ","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:05.127143Z","iopub.execute_input":"2024-09-15T04:41:05.127447Z","iopub.status.idle":"2024-09-15T04:41:05.131986Z","shell.execute_reply.started":"2024-09-15T04:41:05.127413Z","shell.execute_reply":"2024-09-15T04:41:05.131051Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"import torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:05.133227Z","iopub.execute_input":"2024-09-15T04:41:05.133499Z","iopub.status.idle":"2024-09-15T04:41:05.148837Z","shell.execute_reply.started":"2024-09-15T04:41:05.133469Z","shell.execute_reply":"2024-09-15T04:41:05.147821Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:05.150006Z","iopub.execute_input":"2024-09-15T04:41:05.150361Z","iopub.status.idle":"2024-09-15T04:41:05.162669Z","shell.execute_reply.started":"2024-09-15T04:41:05.150328Z","shell.execute_reply":"2024-09-15T04:41:05.161789Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate jiwer","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:05.163775Z","iopub.execute_input":"2024-09-15T04:41:05.164704Z","iopub.status.idle":"2024-09-15T04:41:21.286069Z","shell.execute_reply.started":"2024-09-15T04:41:05.164647Z","shell.execute_reply":"2024-09-15T04:41:21.284834Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer, evaluate\nSuccessfully installed evaluate-0.4.3 jiwer-3.0.4 rapidfuzz-3.9.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"wer\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:21.287850Z","iopub.execute_input":"2024-09-15T04:41:21.288224Z","iopub.status.idle":"2024-09-15T04:41:22.711219Z","shell.execute_reply.started":"2024-09-15T04:41:21.288184Z","shell.execute_reply":"2024-09-15T04:41:22.710377Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"468630993a504aa99254e9e5bbdad92d"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    # we do not want to group tokens when computing the metrics\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:22.712470Z","iopub.execute_input":"2024-09-15T04:41:22.712740Z","iopub.status.idle":"2024-09-15T04:41:22.718624Z","shell.execute_reply.started":"2024-09-15T04:41:22.712709Z","shell.execute_reply":"2024-09-15T04:41:22.717708Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-finetuned\",  # change to a repo name of your choice\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n    learning_rate=1e-5,\n    warmup_steps=500,\n    max_steps=4000,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=1000,\n    eval_steps=1000,\n    logging_steps=25,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    push_to_hub=False,\n    remove_unused_columns=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:22.719567Z","iopub.execute_input":"2024-09-15T04:41:22.719843Z","iopub.status.idle":"2024-09-15T04:41:22.848257Z","shell.execute_reply.started":"2024-09-15T04:41:22.719813Z","shell.execute_reply":"2024-09-15T04:41:22.847241Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=processed_dataset[\"train\"],\n    eval_dataset=processed_dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:22.849483Z","iopub.execute_input":"2024-09-15T04:41:22.849800Z","iopub.status.idle":"2024-09-15T04:41:23.506005Z","shell.execute_reply.started":"2024-09-15T04:41:22.849766Z","shell.execute_reply":"2024-09-15T04:41:23.505103Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"processor.save_pretrained(training_args.output_dir)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-09-15T04:41:23.507132Z","iopub.execute_input":"2024-09-15T04:41:23.507439Z","iopub.status.idle":"2024-09-15T04:41:24.791557Z","shell.execute_reply.started":"2024-09-15T04:41:23.507401Z","shell.execute_reply":"2024-09-15T04:41:24.790612Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"processed_dataset","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:41:24.792909Z","iopub.execute_input":"2024-09-15T04:41:24.793334Z","iopub.status.idle":"2024-09-15T04:41:24.799492Z","shell.execute_reply.started":"2024-09-15T04:41:24.793289Z","shell.execute_reply":"2024-09-15T04:41:24.798559Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_features', 'labels'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['input_features', 'labels'],\n        num_rows: 5128\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Check for missing 'input_features' in the training dataset\nmissing_features = [i for i, example in enumerate(processed_dataset['train']) if 'input_features' not in example]\n\nif missing_features:\n    print(f\"Missing 'input_features' in the following indices: {missing_features}\")\nelse:\n    print(\"All entries have 'input_features'.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install dill","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:48:41.328407Z","iopub.execute_input":"2024-09-15T04:48:41.328958Z","iopub.status.idle":"2024-09-15T04:48:54.245689Z","shell.execute_reply.started":"2024-09-15T04:48:41.328910Z","shell.execute_reply":"2024-09-15T04:48:54.244464Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (0.3.8)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:52:07.361799Z","iopub.execute_input":"2024-09-15T04:52:07.362516Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='788' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 788/4000 1:34:39 < 6:26:51, 0.14 it/s, Epoch 2.51/13]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\n# Assuming 'model' is your fine-tuned Whisper model\nmodel.save_pretrained(\"./whisper_finetuned_AkuapemTwi_model\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:42:46.997655Z","iopub.status.idle":"2024-09-15T04:42:46.998009Z","shell.execute_reply.started":"2024-09-15T04:42:46.997832Z","shell.execute_reply":"2024-09-15T04:42:46.997851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}